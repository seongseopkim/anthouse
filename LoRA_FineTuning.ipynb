{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPAhHWANqOLvy5w/l7uf+TX",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/seongseopkim/anthouse/blob/main/LoRA_FineTuning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "라이브러리 설치"
      ],
      "metadata": {
        "id": "EjzgN7I3k9Ca"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M9ESDNIIkfEc"
      },
      "outputs": [],
      "source": [
        "!pip install -q transformers datasets peft accelerate bitsandbytes\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "BART, 토크나이저 로딩 ( 원래 사용하던 모델 로딩 )"
      ],
      "metadata": {
        "id": "FEhkL3HLlE--"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n",
        "\n",
        "model_id = \"facebook/bart-large-cnn\"\n",
        "\n",
        "# 토크나이저와 모델 로딩\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(model_id)\n",
        "\n",
        "print(\"✅ 모델과 토크나이저 로딩 완료!\")\n"
      ],
      "metadata": {
        "id": "cVerTSSRlf4C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "LoRA 설정"
      ],
      "metadata": {
        "id": "Iu5cAZQdl9PB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from peft import get_peft_model, LoraConfig, TaskType\n",
        "\n",
        "# LoRA 설정 정의\n",
        "lora_config = LoraConfig(\n",
        "    r=8,                        # 랭크: 작을수록 가벼움, 클수록 정밀\n",
        "    lora_alpha=16,             # scaling factor\n",
        "    lora_dropout=0.1,          # dropout 비율\n",
        "    bias=\"none\",               # bias는 LoRA 적용 안 함\n",
        "    task_type=TaskType.SEQ_2_SEQ_LM,  # 시퀀스-투-시퀀스 작업 (요약)\n",
        "    target_modules=[\"q_proj\", \"v_proj\"]  # BART에서 LoRA를 적용할 레이어\n",
        ")\n",
        "\n",
        "# 기존 모델에 LoRA 구성 적용\n",
        "model = get_peft_model(model, lora_config)\n",
        "\n",
        "# 학습되는 파라미터 확인\n",
        "model.print_trainable_parameters()\n"
      ],
      "metadata": {
        "id": "edUbLdL1mAiH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "학습 파라미터 설정"
      ],
      "metadata": {
        "id": "hYkq4K0noRKp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import Seq2SeqTrainingArguments\n",
        "\n",
        "training_args = Seq2SeqTrainingArguments(\n",
        "    output_dir=\"./lora-bart-checkpoints\",   # 결과 저장 경로\n",
        "    per_device_train_batch_size=2,          # 배치 크기 (적은 데이터면 2~4)\n",
        "    per_device_eval_batch_size=2,\n",
        "    learning_rate=1e-4,                     # 학습률\n",
        "    num_train_epochs=3,                     # 학습 epoch 수\n",
        "    logging_dir=\"./logs\",                   # 로그 저장 경로\n",
        "    save_strategy=\"steps\",                  # 몇 step마다 저장할지\n",
        "    evaluation_strategy=\"steps\",            # 평가 주기\n",
        "    eval_steps=100,                         # 평가 간격\n",
        "    save_steps=200,                         # 저장 간격\n",
        "    predict_with_generate=True,             # 요약 결과 생성도 평가에 포함\n",
        "    fp16=True,                              # GPU가 지원하면 float16 사용\n",
        "    report_to=\"none\"                        # WandB 등 외부 로깅 끔\n",
        ")\n",
        "\n",
        "print(\"✅ 학습 파라미터 설정 완료!\")\n"
      ],
      "metadata": {
        "id": "HDiQ63DYoTnS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import Seq2SeqTrainer, DataCollatorForSeq2Seq\n",
        "\n",
        "# 데이터가 들어올 때 padding 등을 자동으로 맞춰주는 콜레이터\n",
        "data_collator = DataCollatorForSeq2Seq(tokenizer, model=model)\n",
        "\n",
        "# 빈 Trainer 준비 (데이터셋은 나중에 연결할 거야)\n",
        "trainer = Seq2SeqTrainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=None,   # 나중에 데이터셋 연결\n",
        "    eval_dataset=None,\n",
        "    tokenizer=tokenizer,\n",
        "    data_collator=data_collator,\n",
        ")\n",
        "\n",
        "print(\"✅ Trainer 객체 생성 완료 (데이터셋은 나중에 연결)\")\n"
      ],
      "metadata": {
        "id": "3M1Oe_zGoqWE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "데이터셋 집어넣기"
      ],
      "metadata": {
        "id": "MSmXikrVsbkc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "import json\n",
        "\n",
        "# Colab에서 파일 업로드\n",
        "uploaded = files.upload()  # 여기서 lora_dataset.jsonl 선택\n",
        "\n",
        "# JSONL 읽기\n",
        "dataset = []\n",
        "with open(\"lora_dataset.jsonl\", \"r\", encoding=\"utf-8\") as f:\n",
        "    for line in f:\n",
        "        dataset.append(json.loads(line))\n",
        "\n",
        "# 확인\n",
        "print(dataset[0])\n"
      ],
      "metadata": {
        "id": "N1bhQi2tseFK"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}